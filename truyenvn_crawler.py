# -*- coding: utf-8 -*-
"""truyenvn_crawler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rkSee20MnBG7vYOB5_WcML5Qp_7SyNUa
"""

import requests
from bs4 import BeautifulSoup

def crawl_truyensextv69(url):
    """Crawls truyensextv69.com and extracts story titles and links."""
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        soup = BeautifulSoup(response.content, 'html.parser')

        story_items = soup.find_all('div', class_='noibat')

        stories = []
        for item in story_items:
            a_tag = item.find('a')
            if a_tag:
                title = a_tag.text.strip()
                link = a_tag['href']
                if "top" not in title.lower() and "danh s√°ch" not in title.lower():
                  stories.append({'title': title, 'link': link})

        return stories

    except requests.exceptions.RequestException as e:
        print(f"Error during request: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

stories = []
base_url = "https://truyensextv69.com"

for i in range(407):
  stories += crawl_truyensextv69(f"{base_url}/page/{i}/")

stories

def crawl_episodes(url):
    response = requests.get(url)
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Parse the HTML content using BeautifulSou
    # Find all the <a> tags with class 'post-page-numbers'
    results = []
    bai_viet_boxes = soup.findAll('div', class_='bai-viet-box')
    for bai_viet_box in bai_viet_boxes:
      if bai_viet_box:
          # Find all <a> tags with class 'post-page-numbers' within the 'bai-viet-box' div
          links = bai_viet_box.find_all('a', class_='post-page-numbers')
          for link in links:
              title = link.text.strip()
              href = link['href']
              results.append({'title': title, 'link': href})

    return results

def crawl_content(url):
    response = requests.get(url)
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
    soup = BeautifulSoup(response.content, 'html.parser')

    ndtruyen = soup.find('div', class_='ndtruyen')
    return ndtruyen.text.strip() if ndtruyen else None

crawl_espodes("https://truyensextv69.com/con-dau-thoi-nay/")
crawl_content("https://truyensextv69.com/con-dau-thoi-nay/")

contents = []


for story in stories:
    episodes = crawl_episodes(story['link'])
    content = crawl_content(story['link'])
    contents.append({"title": story["title"], "content": content, "link": story['link']})

    for episode in episodes:
      content = crawl_content(episode["link"])
      if content:
        contents.append({"title": f"{story['title']} {episode['title']}", "content": content, "link": episode["link"]})

import json

def write_vietnamese_json(content):
      with open(f"data/{content['title']}.json", 'w', encoding='utf-8') as f:
        json.dump(content, f, ensure_ascii=False, indent=4)


for content in contents:
  write_vietnamese_json(content)

import shutil

def zip_folder(folder_path='data', output_zip='data.zip'):
    shutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_path)

zip_folder()